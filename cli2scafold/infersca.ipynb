{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85705c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, DistributedSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import os\n",
    "import pickle\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from sequence_embedding import SequenceToVectorModel\n",
    "\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.0003,\n",
    "    'batch_size': 24,\n",
    "    'mlp_units': 128,\n",
    "    'dropout_rate': 0.1,\n",
    "    'num_epochs': 20,\n",
    "    'd_model': 4,\n",
    "    'd_inner': 48,\n",
    "    'n_ssm': 1,\n",
    "    'dt_rank': 1,\n",
    "    'n_layer': 1,\n",
    "    'dropout': 0.15,\n",
    "    'n_splits': 4,  # actually not used now\n",
    "    'max_seq_length': 4000\n",
    "}\n",
    "\n",
    "amino_acid_to_index = {aa: idx for idx, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
    "device = torch.device(\"cuda\")\n",
    "rdict_path = \"/public/home/kngll/Mambaphase/model/merged_dict.pkl\"\n",
    "\n",
    "max_seq_length = hyperparams['max_seq_length']\n",
    "\n",
    "sequence_model_params = {\n",
    "            'd_model': hyperparams['d_model'],\n",
    "            'd_inner': hyperparams['d_inner'],\n",
    "            'n_ssm': hyperparams['n_ssm'],\n",
    "            'dt_rank': hyperparams['dt_rank'],\n",
    "            'vocab_size': len(\"ACDEFGHIKLMNPQRSTVWYU\"),\n",
    "            'n_layer': hyperparams['n_layer'],\n",
    "            'n_heads': 4,  # 由于超参数未指定n_heads，这里保持原值，如需更改请补充\n",
    "            'output_dim': 192,  # d_inner 一般做output_dim更合适\n",
    "            'dropout': hyperparams['dropout']\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d294712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d004b40666a848ab8dfe1faaf4b9834b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3199637/3153255029.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  esm_model.load_state_dict(torch.load(esm_weight_path), strict=False)\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共1条序列，计算esm表示...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3199637/3153255029.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "esm_weight_path = \"/public/home/kngll/Mambaphase/data/esm2_t36_3B_UR50D_mlm_finetuned.pth\"\n",
    "esm_model_path = \"/public/home/kngll/llps/data/esm2_t36_3B_UR50D\"\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    sequences, labels, rdict_seqs = zip(*batch)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels, 0)\n",
    "    max_length = max(seq.size(0) for seq in rdict_seqs)\n",
    "    padded_rdict_seqs = []\n",
    "    for seq in rdict_seqs:\n",
    "        padded_seq = torch.zeros(max_length)\n",
    "        padded_seq[:seq.size(0)] = seq\n",
    "        padded_rdict_seqs.append(padded_seq)\n",
    "    rdict_seqs_padded = torch.stack(padded_rdict_seqs, 0)\n",
    "    return sequences_padded, labels, rdict_seqs_padded\n",
    "\n",
    "\n",
    "def infer_esm_rep(model, tokenizer, sequence, device):\n",
    "    encoded_inputs = tokenizer(sequence, return_tensors='pt', padding=True, truncation=True)\n",
    "    encoded_inputs = {k: v.to(device) for k, v in encoded_inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        with autocast():\n",
    "            outputs = model(**encoded_inputs, output_hidden_states=True)\n",
    "    representations = outputs.hidden_states[-1]\n",
    "    last_hidden_state = representations[:, 0, :]\n",
    "    torch.cuda.empty_cache()\n",
    "    return last_hidden_state.squeeze(0).cpu()\n",
    "\n",
    "sequences = [\n",
    "    \"MNRYLNRQRLYNMEEERNKYRGVMEPMSRMTMDFQGRYMDSQGRMVDPRYYDHYGRMHDYDRYYGRSMFNQGHSMDSQRYGGWMDNPERYMDMSGYQMDMQGRWMDAQGRYNNPFSQMWHSRQGHYPGEEEMSHHSMYGRNMHYPYHSHSASRHFDSPERWMDMSGYQMDMQGRWMDNYGRYVNPFHHHMYGRNMFYPYGSHCNNRHMEHPERYMDMSGYQMDMQGRWMDTHGRHCNPLGQMWHNRHGYYPGHPHGRNMFQPERWMDMSSYQMDMQGRWMDNYGRYVNPFSHNYGRHMNYPGGHYNYHHGRYMNHPERQMDMSGYQMDMHGRWMDNQGRYIDNFDRNYYDYHMY\",\n",
    "    # 可添加更多序列\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(esm_model_path)\n",
    "esm_model = AutoModelForMaskedLM.from_pretrained(esm_model_path)\n",
    "esm_model.load_state_dict(torch.load(esm_weight_path), strict=False)\n",
    "esm_model = esm_model.to(device)\n",
    "\n",
    "print(f\"共{len(sequences)}条序列，计算esm表示...\")\n",
    "esm_reps = []\n",
    "for seq in sequences:\n",
    "    if len(seq) > 4000:\n",
    "        seq = seq[:4000]\n",
    "    try:\n",
    "        rep = infer_esm_rep(esm_model, tokenizer, seq, device)\n",
    "        esm_reps.append(rep)\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"OOM error! 忽略序列: \", seq[:10], \"...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        esm_reps.append(torch.zeros(2560, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0fe107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载下游分类模型...\n"
     ]
    }
   ],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, sequence_model, rdict_dim=2560, mlp_units=384, dropout_rate=0.2, seq_emb_dim=192):\n",
    "        super().__init__()\n",
    "        self.sequence_model = sequence_model\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(rdict_dim, mlp_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_units, seq_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(seq_emb_dim * 2, mlp_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_units, 2)\n",
    "        )\n",
    "    def forward(self, input_ids, rdict_seqs):\n",
    "        embeddings = self.sequence_model(input_ids)\n",
    "        rdict_embeddings = self.mlp1(rdict_seqs)\n",
    "        combined_embeddings = torch.cat((rdict_embeddings, embeddings), dim=1)\n",
    "        logits = self.classifier(combined_embeddings)\n",
    "        return logits\n",
    "AA_LIST = \"ACDEFGHIKLMNPQRSTVWYU\"\n",
    "def load_cls_model(cls_model_path):\n",
    "    sequence_model = SequenceToVectorModel(\n",
    "        vocab_size=len(AA_LIST),\n",
    "        d_model=hyperparams['d_model'],\n",
    "        d_inner=hyperparams['d_inner'],\n",
    "        n_ssm=hyperparams['n_ssm'],\n",
    "        dt_rank=hyperparams['dt_rank'],\n",
    "        n_layer=hyperparams['n_layer'],\n",
    "        dropout=hyperparams['dropout'],\n",
    "        output_dim=192,\n",
    "        n_heads=4\n",
    "    )\n",
    "    # 关键修改2：显式传递分类模型参数\n",
    "    model = ClassificationModel(\n",
    "        sequence_model=sequence_model,\n",
    "        mlp_units=hyperparams['mlp_units'],\n",
    "        dropout_rate=hyperparams['dropout_rate']\n",
    "    ).to(device)\n",
    "    # 关键修改3：设置weights_only=True并处理加载\n",
    "    state_dict = torch.load(cls_model_path, map_location=device, weights_only=True)\n",
    "    if any(k.startswith(\"module.\") for k in state_dict):\n",
    "        new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    else:\n",
    "        new_state_dict = state_dict\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "cls_model_path = \"/public/home/kngll/Mambaphase/data/modelscaffold/model_epoch19.pth\"\n",
    "\n",
    "print(\"加载下游分类模型...\")\n",
    "cls_model = load_cls_model(cls_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3349fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备分类数据...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class PredictionDatasetV2(Dataset):\n",
    "    def __init__(self, sequences, esm_reps, amino_acid_to_index):\n",
    "        self.sequences = sequences\n",
    "        self.esm_reps = esm_reps\n",
    "        self.amino_acid_to_index = amino_acid_to_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        encoded_seq = torch.tensor([self.amino_acid_to_index.get(aa, 0) for aa in seq], dtype=torch.long)\n",
    "        esm_rep = self.esm_reps[idx]\n",
    "        return encoded_seq, esm_rep, seq\n",
    "def collate_fn_predict(batch):\n",
    "    seqs, esm_reps, origs = zip(*batch)\n",
    "    padded_seqs = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    esm_tensor = torch.stack([r if r.ndim==1 else r.squeeze(0) for r in esm_reps])\n",
    "    return padded_seqs, esm_tensor, origs\n",
    "\n",
    "print(\"准备分类数据...\")\n",
    "dataset = PredictionDatasetV2(sequences, esm_reps, amino_acid_to_index)\n",
    "loader = DataLoader(dataset, batch_size=24, collate_fn=collate_fn_predict, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fdca2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进行预测...\n",
      "已写入: /public/home/kngll/Mambaphase/results/predictionclis.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"进行预测...\")\n",
    "results = []\n",
    "result_csv_path = \"/public/home/kngll/Mambaphase/results/predictionclis.csv\"\n",
    "with torch.no_grad():\n",
    "    for seqs, esm_reps, origs in loader:\n",
    "        seqs = seqs.to(device)\n",
    "        esm_reps = esm_reps.to(device)\n",
    "        outputs = cls_model(seqs, esm_reps)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        for i in range(len(origs)):\n",
    "            results.append({\n",
    "                \"sequence\": origs[i],\n",
    "                \"prob_0\": probs[i][0].item(),\n",
    "                \"prob_1\": probs[i][1].item(),\n",
    "                \"prediction\": torch.argmax(probs[i]).item()\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "os.makedirs(os.path.dirname(result_csv_path), exist_ok=True)\n",
    "df.to_csv(result_csv_path, index=False)\n",
    "print(f\"已写入: {result_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepstabp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
